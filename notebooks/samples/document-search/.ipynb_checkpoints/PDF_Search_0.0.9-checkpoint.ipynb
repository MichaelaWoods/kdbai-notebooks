{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae26489-0212-4067-9524-c68f2dce58c6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b>This notebook uses KDB.AI 0.0.9 which has an temporary version of the Python Client API that will change significantly in the final version for release in September 2023.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280b01a-d3b7-4ef6-9494-789d15bc48ec",
   "metadata": {},
   "source": [
    "# Semantic Search on PDF Documents with KDB.AI\n",
    "\n",
    "This example demonstrates how to use KDB.AI to run semantic search on unstructured text documents. \n",
    "\n",
    "Semantic search allows users to perform searches based on the meaning or similarity of the data rather than exact matches. It works by converting the query into a vector representation and then finding similar vectors in the database. This way, even if the query and the data in the database are not identical, the system can identify and retrieve the most relevant results based on their semantic meaning.\n",
    "\n",
    "## Aim\n",
    "In this tutorial, we'll walk you through the process of performing semantic search on documents, taking PDFs as example, using KDB.AI as the vector store. We will cover the following topics:\n",
    "\n",
    "- How to create vector embeddings using Sentence Transformer\n",
    "- How to store those embeddings in KDB.AI\n",
    "- How to search with a query using KDB.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96697d51-a815-4ee9-acef-555afe28c2b6",
   "metadata": {},
   "source": [
    "## 1. Load and Split Document\n",
    "\n",
    "### Install Dependencies:\n",
    "\n",
    "We first need to install some libraries:\n",
    "- [PyDFF2](https://pypi.org/project/PyPDF2/) which is a useful library when handling PDFs in Python\n",
    "- [spaCy](https://spacy.io/) for advanced natural language processing which will help identify sentences in the PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427beb75-cc0a-4a00-a64b-c7a80e197f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2 -q\n",
    "%pip install spacy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf6847ec-9cef-4648-82b9-b888620f7115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2992812-4705-489d-974f-b7b44132343a",
   "metadata": {},
   "source": [
    "### Load and Split PDF into Sentences\n",
    "\n",
    "We leverage the power of PyPDF2 for PDF processing and spaCy for advanced natural language processing, the code below extracts content from each page of the PDF and processes it to identify sentences.\n",
    "\n",
    "The PDF we are using is [this research paper](https://arxiv.org/pdf/2308.05801.pdf) presenting information on the formation of Interstellar Objects in the Milky Way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cbf9ede-6ed3-4171-bc77-2da673dcff6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def split_pdf_into_sentences(pdf_path):\n",
    "    # Open the PDF file\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        \n",
    "        # Extract text from each page and concatenate\n",
    "        full_text = \"\"\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            full_text += page.extract_text()\n",
    "        \n",
    "        # Process the text using spaCy for sentence tokenization\n",
    "        doc = nlp(full_text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "        \n",
    "        return sentences\n",
    "\n",
    "# Define PDF path\n",
    "pdf_path = 'research_paper.pdf'\n",
    "\n",
    "# Split the PDF into sentences\n",
    "pdf_sentences = split_pdf_into_sentences(pdf_path)\n",
    "len(pdf_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2558747e-4559-44b2-8ecb-88a583cc0ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Draft version August 14, 2023\\nTypeset using L ATEX default style in AASTeX631\\nThe Galactic Interstellar Object Population: A Framework for Prediction and Inference\\nMatthew J. Hopkins\\n ,1Chris Lintott\\n ,1Michele T. Bannister\\n ,'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea4179-8b26-4c40-89aa-ef6da11aafdf",
   "metadata": {},
   "source": [
    "## 2. Create Vector Embeddings \n",
    "\n",
    "Next, we use the Sentence Transformers library to create embeddings for our collection of sentences.\n",
    "\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796f63aa-fba3-467e-8960-a6da6195434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1706a-6046-4dae-9b54-a59d4805f286",
   "metadata": {},
   "source": [
    "### Selecting a Sentence Transformer model\n",
    "\n",
    "There are 100+ different types of Sentence Transformers models available - see [HuggingFace](https://huggingface.co/sentence-transformers) for the full list. The diversity among these primarily stems from variations in their training data. Selecting the ideal model for your needs involves matching the domain and task closely, while also considering the benefits of incorporating larger datasets to enhance scale. \n",
    "\n",
    "This tutorial will use the `all-MiniLM-L6-v2` pre-trained model, this embedding model can create sentence and document embeddings that can be used for a wide variety of tasks including semantic search which makes it a good choice for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "823cc952-139a-49be-b5eb-802554aed708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2a8dedaf824d75ad9fd46f62c137f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c4bc11e19c4e048ef43becbb02fb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a83e698a2904fa18de38a546e5e6816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45724939dac4bf9b2ed28c3d737c506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a84ba75e02f451aa03c2c492599c864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb15fd1cb03f44b18323dba0628ab02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861864bca3d6434aa627d75abebb5431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b7587399ff45ada0e8e9097c954da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f991e58cd8c43e3af570bc34600b2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d932932f00f74a289784437aadbb14cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef3083c695e48be93a58ca801419439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1746b56bab42493cbe3943f567acb66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5c2fc1f2434bc6b43396f3ce5f621e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755aad5ae2ba4cd8ae903a21315afb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer \n",
    "model=SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4aaf7-69c5-4c56-b058-32ced66a62ee",
   "metadata": {},
   "source": [
    "### Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d05b99a-121c-429e-864a-ee4bc5739224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393, 384)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Create embeddings\n",
    "embeddings = model.encode(np.array(pdf_sentences))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112af3d-2fe7-4cf0-ba0d-72816ea19b7e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important:</b>Note the dimension of our embeddings is 384, it is the second value returned from shape. This will need to match the dimensions we set in the KDB.AI index in the next step.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70505eae-4138-4ba8-80e9-fc13c37d0b32",
   "metadata": {},
   "source": [
    "## 3. Store Embeddings in KDB.AI\n",
    "\n",
    "With the embeddings created, we need to store them in a vector database to enable efficient searching. KDB.AI is perfect for this task.\n",
    "\n",
    "### Create a vector index\n",
    "\n",
    "With KDB.AI we have the choice between HNSW (Hierarchical Navigable Small World) and IVFPQ (Inverted File with Product Quantization) indexing methods. Generally, for semantic search of documents, the HNSW indexing method might be more suitable. Here's why:\n",
    "\n",
    "- **Search Speed and Approximation**: HNSW is designed for fast approximate nearest neighbor searches. It can efficiently handle high-dimensional data, which is common in natural language processing tasks involving text documents.\n",
    "- **Semantic Representation**: The Sentence Transformers library, used in this example, generates embeddings that capture semantic meaning. HNSW is well-suited for indexing such embeddings and performing semantic searches.\n",
    "- **Scalability**: HNSW is scalable and can handle large datasets effectively, making it suitable for applications with a vast number of documents.\n",
    "\n",
    "HNSW provides approximate search results, meaning that the nearest neighbors might not be exact matches but are close in terms of similarity. If you require more accurate search results, IVFPQ might be a better choice, albeit at the cost of slightly slower search times and potentially higher memory usage.\n",
    "\n",
    "The command below creates an index named `myHNSW` that performs Hierarchical Navigable Small Worlds (HNSW) for 384-dimensional vectors, which matches the dimensions of our embeddings as identified in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d190db-7c19-418e-9140-3dff04c9d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kdbai\n",
    "index = kdbai.Index('myHNSW', dict(type='hnsw', metric='L2', dims=384, efConstruction=8, M=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a9d0e-80b8-4e09-9101-d22667da551f",
   "metadata": {},
   "source": [
    "### Add embeddings to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83cc156c-8071-4784-8c3e-a049b61e8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.insert(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ddb725-e0e7-4c00-a22b-3234eacf6bd1",
   "metadata": {},
   "source": [
    "## 4. Searching with a Query using KDB.AI\n",
    "\n",
    "Now that the embeddings are stored in KDB.AI, we can perform semantic search using `search`. \n",
    "\n",
    "First, we embed our search term using the Sentence Transformer model as before. Then we search our index to return to 3 most similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8666561e-e9b2-4c9f-95f4-add789be416d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2J. Ted Mackereth\\n ,3, 4, 5, ∗and\\nJohn C. Forbes\\n2\\n1Department of Physics, University of Oxford, Denys Wilkinson Building, Keble Road, Oxford, OX1 3RH, UK\\n2School of Physical and Chemical Sciences—Te Kura Mat¯ u, University of Canterbury, Private Bag 4800, Christchurch 8140, New Zealand\\n3Just Group plc, Enterprise House, Bancroft road, Reigate, Surrey RH2 7RP, UK\\n4Canadian Institute for Theoretical Astrophysics, University of Toronto, 60 St. George Street, Toronto, ON, M5S 3H8, Canada\\n5Dunlap Institute for Astronomy and Astrophysics, University of Toronto, 50 St. George Street, Toronto, ON M5S 3H4, Canada\\nABSTRACT\\nThe Milky Way is thought to host a huge population of interstellar objects (ISOs), numbering\\napproximately 1015pc−3around the Sun, which are formed and shaped by a diverse set of processes\\nranging from planet formation to galactic dynamics.',\n",
       "        'In this work, we develop\\nthis method and apply it to the stellar population of the Milky Way, estimated with data from the APOGEE survey, to\\npredict a broader set of properties of our own Galaxy’s population of interstellar objects.',\n",
       "        'Keywords: Interstellar objects (52), Small Solar System bodies(1469), Galaxy Evolution (594)\\n1.INTRODUCTION\\n1I/‘Oumuamua (Meech et al. 2017) and 2I/Borisov1are the first two observed samples from a highly numerous\\npopulation: interstellar objects (ISOs).']],\n",
       "      dtype='<U5531')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = 'number of interstellar objects in the milky way'\n",
    "\n",
    "search_term_vector = model.encode(search_term)\n",
    "np.array(pdf_sentences)[(index.search( np.array(search_term_vector),3))[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f624af-a151-40e5-9a6f-9e7253f038fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['The pop-\\nulation’s dominant dynamical formation mechanisms would preferentially harvest more distant, ice-rich planetesimals\\nfrom the disks of the source systems.',\n",
       "        'A protoplanetary disk has to first order the same composition as the star it forms around,\\nsince they both form from the same molecular cloud core.',\n",
       "        'While in reality, stars will each produce a distribution of ISOs that\\nformed at different positions in their protoplanetary disk and thus have a range of compositions, this simplification\\nof only modelling planetesimals which form exterior to the water ice line is justified by the proportionally greater\\nreservoir of snowline-exterior planetesimals, and the higher efficiencies of formation mechanisms dynamically stripping\\nthem into the interstellar population (Fitzsimmons et al. 2023).']],\n",
       "      dtype='<U5531')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = 'how does planet formation occur'\n",
    "\n",
    "search_term_vector = model.encode(search_term)\n",
    "np.array(pdf_sentences)[(index.search( np.array(search_term_vector),3))[1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
