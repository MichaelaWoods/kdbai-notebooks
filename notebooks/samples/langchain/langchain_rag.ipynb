{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eeba82",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with LangChain and KDB.AI\n",
    "\n",
    "This example will demonstrate how to use an advanced prompt engineering technique called Retrieval Augmented Generation (RAG), with hands-on examples using Langchain, KDB.AI and various LLMs.\n",
    "\n",
    "## What is RAG and Why Do We Need it?\n",
    "\n",
    "Large Language Models have remarkable capabilities in generating human-like text. These models are found in applications ranging from chatbots to content generation and translation. However, they face a significant challenge in staying up-to-date with recent world events, as they are essentially frozen in time, operating within the static knowledge snapshot captured during their training.\n",
    "\n",
    "To bridge this gap and address the need for specialized, real-time information, the concept of \"Retrieval Augmented Generation\" (RAG) has emerged as a powerful solution. RAG enables these language models to access relevant data from external knowledge bases, enriching their responses with current and contextually accurate information.\n",
    "\n",
    "## Aim\n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "- Utilizing LangChain to create OpenAI embeddings for PDF document\n",
    "- Storing these embeddings in a KDB.AI vector database\n",
    "- Returning search results using various LLM's whose prompts are augmented by the context from the vector database aka. RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba76e9",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "We first need to install some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21505ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipython jupyter pandas pyarrow openai pypdf tiktoken kdbai-client git+https://github.com/KxSystems/langchain.git@KDB.AI#subdirectory=libs/langchain -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb407992",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "\n",
    "Load the various libraries that will be needed in this tutorial, including `getpass` to set API tokens, `kdbai_client` to connect to KDB.AI and all the langchain libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ecd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "import pandas as pd\n",
    "import kdbai_client as kdbai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f72689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import KDBAI\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f273bab",
   "metadata": {},
   "source": [
    "## 2. Set API Keys\n",
    "\n",
    "To follow this example you will need to request both an [OpenAI API Key](https://platform.openai.com/apps) and a [Hugging Face API Token](https://huggingface.co/docs/hub/security-tokens). \n",
    "\n",
    "You can create both for free by registering using the links provided. Once you have the credentials you can add them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159357d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass('OpenAI API Key: ')\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass('Hugging Face API Token: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f921b0",
   "metadata": {},
   "source": [
    "## 3. Data Preparation \n",
    "\n",
    "### Load TXT Document\n",
    "\n",
    "In the below code snippet we read the TXT file and split the document into chunks. This document is a State of the Union message from the President of the United States to the United States Congress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50dc02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the documents we want to prompt an LLM about\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "doc = loader.load()\n",
    "\n",
    "#Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(doc) #split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "texts = [p.page_content for p in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d8c79",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    " \n",
    "We will use OpenAIEmbeddings to embed our document into a format suitable for the vector database. We select `text-embedding-ada-002` for use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "508b4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6d6e2",
   "metadata": {},
   "source": [
    "## 4. Use KDB.AI as Vector Database\n",
    "\n",
    "### Connect to KDB.AI\n",
    "\n",
    "To use KDB.AI, you will need two session details - a URL endpoint and an API key. To get these you can sign up for free [here](https://trykdb.kx.com/kdbai/signup).\n",
    "\n",
    "You can connect to a KDB.AI session using `kdbai.Session`. Enter the session URL endpoint and API key details from your KDB.AI Cloud portal below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect to KDB.AI\n",
    "print('Connect KDB.AI session...')\n",
    "KDBAI_ENDPOINT = input('KDB.AI endpoint: ')\n",
    "KDBAI_API_KEY = getpass('KDB.AI API key: ')\n",
    "session = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596eaaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create table \"rag_langchain\"...\n"
     ]
    }
   ],
   "source": [
    "## Setup table for vector store later \n",
    "schema_rag = {'columns': [ {'name': 'id', 'pytype': 'str'},\n",
    "                           {'name': 'text', 'pytype': 'bytes'},\n",
    "                           {'name': 'embeddings',\n",
    "                               'pytype': 'float32',\n",
    "                               'vectorIndex': {'dims': 1536, 'metric': 'L2', 'type': 'flat'}}]}\n",
    "\n",
    "print('Create table \"rag_langchain\"...')\n",
    "table = session.create_table('rag_langchain', schema_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b465157",
   "metadata": {},
   "source": [
    "### Store in KDB.AI\n",
    "\n",
    "We can now store our data in KDB.AI by passing a few parameters to `KDBAI.from_texts`:\n",
    "\n",
    "- `session` our handle to talk to KDB.AI\n",
    "- `table_name` our KDB.AI table name\n",
    "- `texts` the chunked document \n",
    "- `embeddings` the embeddings model we have chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a92e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KDBAI as vector store  \n",
    "\n",
    "vecdb_kdbai = KDBAI.from_texts(session, 'rag_langchain', texts=texts, embedding=embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a28271",
   "metadata": {},
   "source": [
    "Now we have the vector embeddings stored in KDB.AI we are ready to query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb116f8",
   "metadata": {},
   "source": [
    "## 5. Similarity Search \n",
    "\n",
    "Before we implement RAG, let's see an example of using similarity search directly on KDB.AI vector store. The search uses Euclidean similarity search which measures distance between two points in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63aee736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strongâ€”because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today.', metadata={'id': '958497bd-21f0-4d51-8934-68a6469a3d65', 'embeddings': array([-2.1954027e-03, -1.8670579e-02,  4.0691000e-05, ...,\n",
      "       -9.3835755e-04,  7.5132987e-03, -2.0782286e-02], dtype=float32)})]\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the nations strengths?\"\n",
    "\n",
    "query_sim = vecdb_kdbai.similarity_search(query) #query_sim holds results of the similarity search, the closest related chunks to the query.\n",
    "print(query_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ecfec",
   "metadata": {},
   "source": [
    "This result returns the most similar chunks of text to our query, which is an okay start but it is hard to read. It would be a lot better if we could summarize these findings and return a response that is more human readable - this is where RAG comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa29b8",
   "metadata": {},
   "source": [
    "## 6. Retrieval Augmented Generation \n",
    "\n",
    "There are four different ways to do [question answering (QA) in LangChain](https://python.langchain.com/docs/use_cases/question_answering/#go-deeper-4):\n",
    "- `load_qa_chain` will do QA over all documents passed \n",
    "- `RetrievalQA` retrieves the most relevant chunk of text and does QA on that subset. Uses `load_qa_chain` under the hood.\n",
    "- `VectorstoreIndexCreator` is a higher level wrapper for `RetrievalQA` to make it easier to run in fewer lines of code\n",
    "- `ConversationalRetrievalChain` builds on RetrievalQAChain to provide a chat history component\n",
    "\n",
    "In this tutorial we will implement the first two.\n",
    "\n",
    "### load_qa_chain with OpenAI and HuggingFace LLMs\n",
    "\n",
    "We set up two question-answering chains for different models, OpenAI and HuggingFaceHub, using LangChain's `load_qa_chain` function. To do this we first perform the same similarity search run earlier and then run both chains on the query and the related chunks from the documentation, printing the responses from both models. We compare the responses of OpenAI and HuggingFaceHub models to the query about vector database strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f29a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select two llm models (OpenAI text-davinci-003, HuggingFaceHub google/flan-t5-xxl(designed for short answers)) \n",
    "llm_openai = OpenAI(model=\"text-davinci-003\", max_tokens=512)\n",
    "llm_flan = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22959c2",
   "metadata": {},
   "source": [
    "\n",
    "We chose the `chain_type =\"stuff\"` which is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f590804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response: \n",
      " The American people are strong and capable of turning any crisis into an opportunity. \n",
      "\n",
      "HuggingFaceHub Response: \n",
      "possibilities\n"
     ]
    }
   ],
   "source": [
    "#create the chain for each model using langchain load_qa_chain\n",
    "chain_openAI = load_qa_chain(llm_openai, chain_type=\"stuff\")\n",
    "chain_HuggingFaceHub = load_qa_chain(llm_flan, chain_type=\"stuff\")\n",
    "\n",
    "query = \"what are the nations strengths?\"\n",
    "\n",
    "query_sim = vecdb_kdbai.similarity_search(query) #Gather the most related chunks to the query\n",
    "\n",
    "#run the chain on the query and the related chunks from the documentation\n",
    "print(\"OpenAI Response: \")\n",
    "print(chain_openAI.run(input_documents=query_sim, question=query),'\\n')\n",
    "print(\"HuggingFaceHub Response: \")\n",
    "print(chain_HuggingFaceHub.run(input_documents=query_sim, question=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c74a9",
   "metadata": {},
   "source": [
    "We can see the response from OpenAI is longer and more detailed and seems to have done a better job summarizing the nation's strengths from the document provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67516d6",
   "metadata": {},
   "source": [
    "### RetrievalQA with GPT-3.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b79d56",
   "metadata": {},
   "source": [
    "Let's try the second method using `RetrievalQA`. This time lets use GPT-3.5 as our LLM of choice.\n",
    "\n",
    "The code below defines a question-answering bot that combines OpenAI's GPT-3.5 Turbo for generating responses and a retriever that accesses the KDB.AI vector database to find relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddaa8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "qabot = RetrievalQA.from_chain_type(chain_type='stuff',\n",
    "                                    llm=ChatOpenAI(model='gpt-3.5-turbo-16k', temperature=0.0), \n",
    "                                    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K)),\n",
    "                                    return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d1ba3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "what are the nations strengths?\n",
      "\n",
      "The passage mentions several strengths of the United States:\n",
      "\n",
      "1. Resilience and ability to turn crises into opportunities.\n",
      "2. Strong and united American people.\n",
      "3. History of debating and addressing great questions and achieving great things.\n",
      "4. Commitment to freedom, liberty, and democracy.\n",
      "5. Strong military capabilities and commitment to defending NATO allies.\n",
      "6. Mobilization of ground forces, air squadrons, and ship deployments to protect NATO countries.\n",
      "7. Leadership in releasing petroleum reserves to stabilize gas prices.\n",
      "8. Confidence in overcoming challenges and capacity to succeed.\n",
      "9. Support for democracy and peace, with democracies rising to the occasion.\n",
      "10. Diplomatic influence and resolve.\n",
      "11. Preparedness and coordination with allies in responding to threats.\n",
      "12. Imposing economic sanctions on Russia and isolating Putin from the world.\n",
      "13. Unity and determination of the Ukrainian people in the face of aggression.\n",
      "14. Support from fellow Ukrainian Americans and deep bonds between the two nations.\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n\\n{query}\\n')\n",
    "print(qabot(dict(query=query))['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a3d7a",
   "metadata": {},
   "source": [
    "Trying another query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1b517b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "what are the things this country needs to protect?\n",
      "\n",
      "Based on the provided context, the country needs to protect the following:\n",
      "\n",
      "1. The right to vote and ensure that every vote is counted.\n",
      "2. The torch of liberty and the values that have attracted generations of immigrants to the United States.\n",
      "3. The pathway to citizenship for Dreamers, temporary status holders, farm workers, and essential workers.\n",
      "4. The border security by implementing new technology, joint patrols, and dedicated immigration judges.\n",
      "5. American industries and jobs by buying American products and leveling the playing field with competitors like China.\n",
      "6. Communities by investing in crime prevention, community police officers, and holding law enforcement accountable.\n",
      "7. Access to healthcare, including preserving a woman's right to choose and advancing maternal health care.\n",
      "8. LGBTQ+ rights by passing the bipartisan Equality Act.\n",
      "9. Democracy by protecting it from threats and ensuring fairness and opportunity for all.\n",
      "10. American jobs and businesses by using taxpayer dollars to support American products and industries.\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the things this country needs to protect?\"\n",
    "print(f'\\n\\n{query}\\n')\n",
    "print(qabot(dict(query=query))['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7574cc",
   "metadata": {},
   "source": [
    "Clearly, Retrieval Augmented Generation stands out as a valuable technique that synergizes the capabilities of language models such as GPT-3 with the potency of information retrieval.\n",
    "By enhancing the input with contextually specific data, RAG empowers language models to produce responses that are not only more precise but also well-suited to the context. \n",
    "Particularly in enterprise scenarios where extensive fine-tuning may not be feasible, RAG presents an efficient and economically viable approach to deliver personalized and informed interactions with users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
