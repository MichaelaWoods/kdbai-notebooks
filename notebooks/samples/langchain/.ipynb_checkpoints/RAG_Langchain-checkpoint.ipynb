{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eeba82",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) wih LangChain and KDB.AI\n",
    "\n",
    "This example will demonstrate how to use an advanced prompt engineering technique called retrieval augmented generation, with hands-on examples using Langchain, KDB.AI and various LLMs.\n",
    "\n",
    "## What is RAG and why do we need it?\n",
    "\n",
    "Large language models have remarkable capabilities in generating human-like text. These models have found applications ranging from chatbots to content generation and translation. However, they face a significant challenge in staying up-to-date with recent world events, as they are essentially frozen in time, operating within the static knowledge snapshot captured during their training.\n",
    "\n",
    "To bridge this gap and address the need for specialized, real-time information, the concept of \"Retrieval Augmented Generation\" (RAG) has emerged as a powerful solution. RAG enables these language models to access relevant data from external knowledge bases, enriching their responses with current and contextually accurate information.\n",
    "\n",
    "\n",
    "## Aim\n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "- Utilizing LangChain to create OpenAI embeddings for PDF document\n",
    "- Storing these embeddings in a KDB.AI vector database\n",
    "- Returning search results using various LLM's whose prompts are augmented by the context from the vector database aka. RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba76e9",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "We first need to install some libraries:\n",
    "- [openai](https://pypi.org/project/openai/) Framework to access OpenAI\n",
    "- [token](https://pypi.org/project/token/) Tokenizer framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21505ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai tiktoken -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de88374",
   "metadata": {},
   "source": [
    "### Install KDB.AI Langchain plugin\n",
    "\n",
    "To use KDB.AI as a vector store for Langchain we first need to get and install the library, clone/download this [repository][####ADD LINK] and then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb407992",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "\n",
    "Load the various libraries that will be needed in this tutorial, including `getpass` to set API tokens, `kdbai_client` to connect to KDB.AI and all the langchain libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ecd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "import pandas as pd\n",
    "import kdbai_client as kdbai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f72689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import KDBAI\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f273bab",
   "metadata": {},
   "source": [
    "## Set API Keys\n",
    "\n",
    "To follow this example you will need to request both an [OpenAI API Key](https://platform.openai.com/apps) and a [Hugging Face API Token](https://huggingface.co/docs/hub/security-tokens). \n",
    "\n",
    "You can create both for free by registering and the links provided, once you have the credentials you can add them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "159357d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ········\n",
      "Hugging Face API Token: ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass('OpenAI API Key: ')\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass('Hugging Face API Token: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f921b0",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "### Load TXT Document\n",
    "\n",
    "In the below code snippet we read the TXT file and split the document into chunks. This document is a State of the Union message from the president of the United States to the United States Congress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50dc02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the documents we want to prompt an LLM about\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "doc = loader.load()\n",
    "\n",
    "#Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(doc) #split_documents produces a list of all the chunks created, printing out first chunk for example\n",
    "texts = [p.page_content for p in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d8c79",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    " \n",
    "We will use OpenAIEmbeddings to embed our document into a format suitable for the vector database, we select `text-embedding-ada-002` for use in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "508b4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6d6e2",
   "metadata": {},
   "source": [
    "## Use KDB.AI as Vector Database\n",
    "\n",
    "### Connect to KDB.AI\n",
    "\n",
    "To use KDB.AI, you will need two session details - a hostname and an API key. To get these you can sign up for free [here](#add_link)[ADD LINK TO DO]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "596eaaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect KDB.AI session...\n",
      "Create table \"rag_langchain\"...\n"
     ]
    }
   ],
   "source": [
    "## Connect to KDB.AI\n",
    "print('Connect KDB.AI session...')\n",
    "# session = kdbai.Session(host='XXXX.kx.com', api_key=key)\n",
    "session = kdbai.Session(host='localhost', port=8082, protocol='http')\n",
    "\n",
    "## Setup table for vector store later \n",
    "schema_rag = {'columns': [ {'name': 'id', 'pytype': 'str'},\n",
    "                           {'name': 'text', 'pytype': 'bytes'},\n",
    "                           {'name': 'embeddings',\n",
    "                               'pytype': 'float32',\n",
    "                               'vectorIndex': {'dims': 1536, 'metric': 'L2', 'type': 'flat'}}]}\n",
    "\n",
    "print('Create table \"rag_langchain\"...')\n",
    "table = session.create_table('rag_langchain', schema_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b465157",
   "metadata": {},
   "source": [
    "### Store in KDB.AI\n",
    "\n",
    "We can now store our data in KDB.AI by passing a few parameters to `KDBAI.from_texts`:\n",
    "\n",
    "- `session` our handle to talk to KDB.AI\n",
    "- `table_name` our KDB.AI table name\n",
    "- `texts` the chunked document \n",
    "- `embeddings` the embeddings model we have chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47a92e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KDBAI as vector store  \n",
    "vecdb_kdbai = KDBAI.from_texts(session, 'rag_langchain', texts=texts, embedding=embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a28271",
   "metadata": {},
   "source": [
    "Now we have the vector embeddings stored in KDB.AI we are ready to query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb116f8",
   "metadata": {},
   "source": [
    "## Similarity Search \n",
    "\n",
    "Before we implement RAG, let's see an example of using similarity search directly on KDB.AI vector store. The search uses Euclidean similarity search, which measures distance between two points in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63aee736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today.', metadata={'id': '965e5a18-b486-4e75-a07c-42f85fe41876', 'embeddings': array([-2.1954027e-03, -1.8670579e-02,  4.0691000e-05, ...,\n",
      "       -9.3835755e-04,  7.5132987e-03, -2.0782286e-02], dtype=float32)}), Document(page_content='In this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself.', metadata={'id': '92785bce-3138-431b-ac19-b4912da4bf64', 'embeddings': array([-0.00685119, -0.02244467, -0.02521917, ..., -0.00772803,\n",
      "        0.00289719, -0.01309381], dtype=float32)}), Document(page_content='It is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.  \\n\\nWe will meet the test. \\n\\nTo protect freedom and liberty, to expand fairness and opportunity. \\n\\nWe will save democracy. \\n\\nAs hard as these times have been, I am more optimistic about America today than I have been my whole life. \\n\\nBecause I see the future that is within our grasp. \\n\\nBecause I know there is simply nothing beyond our capacity.', metadata={'id': 'c0f64585-0999-4fef-ad07-1916bd7bb185', 'embeddings': array([ 0.00015003, -0.03167007, -0.02828783, ..., -0.01801299,\n",
      "       -0.00165269, -0.01992191], dtype=float32)}), Document(page_content='Now is our moment to meet and overcome the challenges of our time. \\n\\nAnd we will, as one people. \\n\\nOne America. \\n\\nThe United States of America. \\n\\nMay God bless you all. May God protect our troops.', metadata={'id': '2f745047-75c9-498f-a89b-b1b3fb0169b8', 'embeddings': array([-0.01012893, -0.03456198, -0.01326978, ..., -0.01749543,\n",
      "       -0.00014733, -0.00920181], dtype=float32)})]\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the nations strengths?\"\n",
    "\n",
    "query_sim = vecdb_kdbai.similarity_search(query) #query_sim holds results of the similarity search, the closest related chunks to the query.\n",
    "print(query_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ecfec",
   "metadata": {},
   "source": [
    "This result returns the most similar chunks of text to our query, which is an okay start but it is hard to read this. It would be a lot better if we could summarize these findings and return a response that is more human readable - this is where RAG comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa29b8",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation \n",
    "\n",
    "There are four different ways to do [question answering in LangChain](https://python.langchain.com/docs/use_cases/question_answering/#go-deeper-4):\n",
    "- `load_qa_chain` will do QA over all documents passed \n",
    "- `RetrievalQA` retreives the most relevant chunk of text and does QA on that subset. Uses `load_qa_chain` under the hood.\n",
    "- `VectorstoreIndexCreator` is a higher level wrapper for `RetrievalQA` to make it easier to run in fewer lines of code\n",
    "- `ConversationalRetrievalChain` builds on RetrievalQAChain to provide a chat history component\n",
    "\n",
    "In this tutorial we will implement the first two.\n",
    "\n",
    "### load_qa_chain with OpenAI and HuggingFace LLMs\n",
    "\n",
    "We set up two question-answering chains for different models, OpenAI and HuggingFaceHub, using LangChain's `load_qa_chain` function. To do this we first perform the same similarity search run earlier and then run both chains on the query and the related chunks from the documentation, printing the responses from both models. We compare the responses of OpenAI and HuggingFaceHub models to the query about vector database strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f29a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select two llm models (OpenAI text-davinci-003, HuggingFaceHub google/flan-t5-xxl(designed for short answers)) \n",
    "llm_openai = OpenAI(model=\"text-davinci-003\", max_tokens=512)\n",
    "llm_flan = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22959c2",
   "metadata": {},
   "source": [
    "\n",
    "We chose the `chain_type =\"stuff\"` which is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f590804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response: \n",
      " The nation is strong because of the American people. It is a nation of possibilities with a history of fighting for freedom and expanding liberty. It is the strongest, freest and most prosperous nation in the world. It has the capacity to meet and overcome the challenges of our time. \n",
      "\n",
      "HuggingFaceHub Response: \n",
      "We have fought for freedom, expanded liberty, defeated totalitarianism and terror.\n"
     ]
    }
   ],
   "source": [
    "#create the chain for each model using langchain load_qa_chain\n",
    "chain_openAI = load_qa_chain(llm_openai, chain_type=\"stuff\")\n",
    "chain_HuggingFaceHub = load_qa_chain(llm_flan, chain_type=\"stuff\")\n",
    "\n",
    "query = \"what are the nations strengths?\"\n",
    "\n",
    "query_sim = vecdb_kdbai.similarity_search(query) #Gather the most related chunks to the query\n",
    "\n",
    "#run the chain on the query and the related chunks from the documentation\n",
    "print(\"OpenAI Response: \")\n",
    "print(chain_openAI.run(input_documents=query_sim, question=query),'\\n')\n",
    "print(\"HuggingFaceHub Response: \")\n",
    "print(chain_HuggingFaceHub.run(input_documents=query_sim, question=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c74a9",
   "metadata": {},
   "source": [
    "We can see the response from OpenAI is longer and more detailed and seems to have done a better job summarizing the strengths of a vector database from the document provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67516d6",
   "metadata": {},
   "source": [
    "### RetrievalQA with GPT-3.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b79d56",
   "metadata": {},
   "source": [
    "Let's try the second method using `RetrievalQA`. This time lets use GPT-3.5 as our LLM of choice.\n",
    "\n",
    "The code below defines a question-answering bot that combines OpenAI's GPT-3.5 Turbo for generating responses and a retriever that accesses the KDB.AI vector database to find relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ddaa8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "qabot = RetrievalQA.from_chain_type(chain_type='stuff',\n",
    "                                    llm=ChatOpenAI(model='gpt-3.5-turbo-16k', temperature=0.0), \n",
    "                                    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K)),\n",
    "                                    return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d1ba3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "what are the nations strengths?\n",
      "\n",
      "The passage mentions several strengths of the United States:\n",
      "\n",
      "1. Resilience and ability to turn crises into opportunities.\n",
      "2. Strong and united American people.\n",
      "3. History of debating and addressing great questions and achieving great things.\n",
      "4. Commitment to freedom, liberty, and democracy.\n",
      "5. Strong military capabilities and commitment to defending NATO allies.\n",
      "6. Mobilization of ground forces, air squadrons, and ship deployments to protect NATO countries.\n",
      "7. Leadership in releasing petroleum reserves to stabilize gas prices.\n",
      "8. Confidence in overcoming challenges and capacity to succeed.\n",
      "9. Support for democracy and peace, with democracies rising to the occasion.\n",
      "10. Diplomatic influence and resolve.\n",
      "11. Preparedness and coordination with allies in responding to threats.\n",
      "12. Imposing economic sanctions on Russia and isolating Putin from the world.\n",
      "13. Unity and determination of the Ukrainian people.\n",
      "\n",
      "These strengths contribute to the overall resilience, prosperity, and influence of the United States.\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n\\n{query}\\n')\n",
    "print(qabot(dict(query=query))['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a3d7a",
   "metadata": {},
   "source": [
    "Trying another query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a1b517b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "what are the things this country needs to protect?\n",
      "\n",
      "Based on the provided context, the country needs to protect:\n",
      "\n",
      "1. The right to vote and ensure that elections are not subverted or suppressed.\n",
      "2. The torch of liberty and the values that have attracted generations of immigrants.\n",
      "3. The pathway to citizenship for Dreamers, temporary status individuals, farm workers, and essential workers.\n",
      "4. Border security and the detection of drug smuggling and human trafficking.\n",
      "5. American industries and jobs by buying American-made products and leveling the playing field with competitors like China.\n",
      "6. Communities by investing in crime prevention, community police officers, and holding law enforcement accountable.\n",
      "7. Neighborhood safety by cracking down on gun trafficking, implementing universal background checks, and banning assault weapons and high-capacity magazines.\n",
      "8. Access to healthcare, including preserving a woman's right to choose and advancing maternal health care.\n",
      "9. Equality for LGBTQ+ Americans by passing the bipartisan Equality Act.\n",
      "10. Democracy by saving it from threats and challenges.\n",
      "11. Mental health services and support for all Americans.\n",
      "12. Veterans and their families by equipping them for war and providing care when they return home.\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the things this country needs to protect?\"\n",
    "print(f'\\n\\n{query}\\n')\n",
    "print(qabot(dict(query=query))['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7574cc",
   "metadata": {},
   "source": [
    "Clearly, Retrieval Augmented Generation stands out as a valuable technique that synergizes the capabilities of language models such as GPT-3 with the potency of information retrieval. By enhancing the input with contextually specific data, RAG empowers language models to produce responses that are not only more precise but also well-suited to the context. Particularly in enterprise scenarios where extensive fine-tuning may not be feasible, RAG presents an efficient and economically viable approach to deliver personalized and informed interactions with users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
