{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c27d502-b9c7-43e3-b4cb-e1cac8630129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load API keys for OpenAI and HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49434983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = getpass('OpenAI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e70efd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face API Token: ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass('Hugging Face API Token: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "534baad9-b7f6-4fc0-85b0-1d8ffbb00b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the documents we want to prompt an LLM about\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('./vector_db_documentation.txt')\n",
    "#loader = TextLoader('./State_of_the_union.txt')\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d86a36b5-ff4f-4de4-b650-08646c96f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Algorithms Several algorithms can facilitate the creation of a vector index. Their common goal is to enable fast querying by creating a data structure that can be traversed quickly. They will commonly transform the representation of the original vector into a compressed form to optimize the query process.  However, as a user of Pinecone, you don't need to worry about the intricacies and selection of these various algorithms. Pinecone is designed to handle all the complexities and algorithmic\" metadata={'source': './vector_db_documentation.txt'}\n"
     ]
    }
   ],
   "source": [
    "#Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "print(chunks[0]) #split_documents produces a list of all the chunks created, printing out first chunk for example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65e13ac0-95c1-46fd-a783-c8391ee0376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The chunks must now be embedded and stored in a vector store\n",
    "#Here we use HuggingFaceEmbeddings to embed the chunks, and FAISS (Facebook AI Similarity Search) as a vector store where the embedded vectors are stored\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vec_db = FAISS.from_documents(chunks, embeddings) # vec_db is the vector store where the embeddings are stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13c6cee1-7f8d-4b30-82e5-19a6372a4a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='the emergence of vector databases as the computation engine that allows us to interact effectively with vector embeddings in our applications.  Vector databases are purpose-built databases that are specialized to tackle the problems that arise when managing vector embeddings in production scenarios. For that reason, they offer significant advantages over traditional scalar-based databases and standalone vector indexes.  In this post, we reviewed the key aspects of a vector database, including', metadata={'source': './vector_db_documentation.txt'}), Document(page_content=\"speed up the filtering tasks. Balancing the trade-offs between search performance and filtering accuracy is essential for providing efficient and relevant query results in vector databases.  Database Operations Unlike vector indexes, vector databases are equipped with a set of capabilities that makes them better qualified to be used in high scale production settings. Let's take a look at an overall overview of the components that are involved in operating the database.  Performance and Fault\", metadata={'source': './vector_db_documentation.txt'}), Document(page_content='which can later be used to populate new indexes.  API and SDKs This is where the rubber meets the road: Developers who interact with the database want to do so with an easy-to-use API, using a toolset that is familiar and comfortable. By providing a user-friendly interface, the vector database API layer simplifies the development of high-performance vector search applications.  In addition to the API, vector databases would often provide programming language specific SDKs that wrap the API. The', metadata={'source': './vector_db_documentation.txt'}), Document(page_content=\"activities within the vector database. This information is crucial for auditing purposes, and when security breaches happen, it helps trace back any unauthorized access or modifications.  Scalability and flexibility: As organizations grow and evolve, their access control needs may change. A robust access control system allows for seamless modification and expansion of user permissions, ensuring that data security remains intact throughout the organization's growth.   Backups and collections\", metadata={'source': './vector_db_documentation.txt'})]\n"
     ]
    }
   ],
   "source": [
    "#An example of using similarity search directly on FAISS vector store\n",
    "#The search uses Euclidean similarity search, which measures distance between two points in vector space. Measures how similar vectors are\n",
    "query = \"what are the strengths of a vector database\"\n",
    "#query = \"what did president obama say about the nations strengths?\"\n",
    "\n",
    "query_sim = vec_db.similarity_search(query) #query_sim holds results of the similarity search, the closest related chunks to the query.\n",
    "print(query_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06f60b94-6c0a-4d48-990c-4491fc74b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get requirements for question answer chain with LLM\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "#instantiate two llm models (OpenAI text-davinci-003, HuggingFaceHub google/flan-t5-xxl(designed for short answers)) \n",
    "llm_openai = OpenAI(model=\"text-davinci-003\", max_tokens=512)\n",
    "llm_flan = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4d8bd50-bb42-4a27-bc91-7149ad6cbd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'max_tokens': 512, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.5, 'max_length': 512}}\n"
     ]
    }
   ],
   "source": [
    "print(llm_openai)\n",
    "print(llm_flan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99d25451-b550-4fdc-af2f-80ca477d7270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response: \n",
      " Vector databases are purpose-built databases that are specialized to tackle the problems that arise when managing vector embeddings in production scenarios. They offer significant advantages over traditional scalar-based databases and standalone vector indexes, such as the ability to speed up filtering tasks, balancing the trade-offs between search performance and filtering accuracy, and providing a user-friendly API and SDKs. \n",
      "\n",
      "HuggingFaceHub Response: \n",
      "purpose-built databases that are specialized to tackle the problems that arise when managing vector embedding\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "#create the chain for each model using langchain load_qa_chain\n",
    "chain_openAI = load_qa_chain(llm_openai, chain_type=\"stuff\")\n",
    "chain_HuggingFaceHub = load_qa_chain(llm_flan, chain_type=\"stuff\")\n",
    "\n",
    "#example prompts\n",
    "#query = \"what year of presidency is this for Obama?\"\n",
    "#query = \"what did president obama say about the nations strengths?\"\n",
    "#query = \"what are the four big questions the country needs to answer?\"\n",
    "\n",
    "#query = \"what is a vector database?\"\n",
    "query = \"what are the strengths of a vector database?\"\n",
    "#query = \"when should I use a vector store?\"\n",
    "\n",
    "query_sim = vec_db.similarity_search(query) #Gather the most related chunks to the query\n",
    "\n",
    "#run the chain on the query and the related chunks from the documentation\n",
    "print(\"OpenAI Response: \")\n",
    "print(chain_openAI.run(input_documents=query_sim, question=query),'\\n')\n",
    "print(\"HuggingFaceHub Response: \")\n",
    "print(chain_HuggingFaceHub.run(input_documents=query_sim, question=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9a92e",
   "metadata": {},
   "source": [
    "## Using KDB.AI as vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a465e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect KDB.AI session...\n",
      "Create table \"rag_langchain4\"...\n"
     ]
    }
   ],
   "source": [
    "## Connect to KDB.AI\n",
    "import kdbai_client as kdbai\n",
    "from langchain.vectorstores import KDBAI\n",
    "\n",
    "print('Connect KDB.AI session...')\n",
    "session = kdbai.Session(host='localhost', port=8082, protocol='http')\n",
    "\n",
    "## Setup table for vector store later \n",
    "schema_rag = {'columns': [ {'name': 'id', 'pytype': 'str'},\n",
    "                           {'name': 'text', 'pytype': 'bytes'},\n",
    "                           {'name': 'embeddings',\n",
    "                               'pytype': 'float32',\n",
    "                               'vectorIndex': {'dims': 1536, 'metric': 'L2', 'type': 'flat'}}]}\n",
    "\n",
    "print('Create table \"rag_langchain4\"...')\n",
    "table = session.create_table('rag_langchain4', schema_rag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c2e0fe28",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "from_texts() missing 2 required positional arguments: 'texts' and 'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vec_db_kdbai \u001b[38;5;241m=\u001b[39m \u001b[43mKDBAI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# use KDBAI as vector store with KDBAI \u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain/vectorstores/base.py:417\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    416\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: from_texts() missing 2 required positional arguments: 'texts' and 'embedding'"
     ]
    }
   ],
   "source": [
    "vec_db_kdbai = KDBAI.from_documents(chunks, embeddings) # use KDBAI as vector store with KDBAI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c140388",
   "metadata": {},
   "source": [
    "`from_documents` doesn't work with KDBAI so need to use `from_texts`. \n",
    "\n",
    "For this to work we need to edit incoming data to load as strings instead of langchain.schema.document.Document as get error: AttributeError: 'Document' object has no attribute 'replace'.  Also Hugging Face embeddings did not work with KDB.AI - switched to using Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0d469aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [p.page_content for p in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c0c3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ff9e8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_db_kdbai = KDBAI.from_texts(session, 'rag_langchain4', texts=texts, embedding=embeddings) # use KDBAI as vector store with KDBAI \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ccd5c64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='the emergence of vector databases as the computation engine that allows us to interact effectively with vector embeddings in our applications.  Vector databases are purpose-built databases that are specialized to tackle the problems that arise when managing vector embeddings in production scenarios. For that reason, they offer significant advantages over traditional scalar-based databases and standalone vector indexes.  In this post, we reviewed the key aspects of a vector database, including', metadata={'id': '4c7956c9-2c34-4b80-b34b-a0bd749ff6cb', 'embeddings': array([-0.03860705, -0.00918662,  0.00900877, ..., -0.02177974,\n",
      "       -0.02815496, -0.00829053], dtype=float32)}), Document(page_content=\"speed up the filtering tasks. Balancing the trade-offs between search performance and filtering accuracy is essential for providing efficient and relevant query results in vector databases.  Database Operations Unlike vector indexes, vector databases are equipped with a set of capabilities that makes them better qualified to be used in high scale production settings. Let's take a look at an overall overview of the components that are involved in operating the database.  Performance and Fault\", metadata={'id': '8584d5a9-c505-49cc-87f4-083f619e5624', 'embeddings': array([-0.0287744 ,  0.01817112, -0.00811532, ..., -0.0116151 ,\n",
      "       -0.01241901, -0.00902319], dtype=float32)}), Document(page_content=\"activities within the vector database. This information is crucial for auditing purposes, and when security breaches happen, it helps trace back any unauthorized access or modifications.  Scalability and flexibility: As organizations grow and evolve, their access control needs may change. A robust access control system allows for seamless modification and expansion of user permissions, ensuring that data security remains intact throughout the organization's growth.   Backups and collections\", metadata={'id': '60c5bbbc-f640-45be-88fe-dd4b425def42', 'embeddings': array([-0.00624322, -0.01072873, -0.00680771, ..., -0.01093154,\n",
      "       -0.00812599, -0.00898455], dtype=float32)}), Document(page_content='which can later be used to populate new indexes.  API and SDKs This is where the rubber meets the road: Developers who interact with the database want to do so with an easy-to-use API, using a toolset that is familiar and comfortable. By providing a user-friendly interface, the vector database API layer simplifies the development of high-performance vector search applications.  In addition to the API, vector databases would often provide programming language specific SDKs that wrap the API. The', metadata={'id': '2499ddee-6792-467a-850f-600d21d20ad1', 'embeddings': array([-0.01319597, -0.00411939, -0.00771039, ..., -0.00406725,\n",
      "       -0.04805612, -0.00850299], dtype=float32)})]\n"
     ]
    }
   ],
   "source": [
    "#An example of using similarity search directly on FAISS vector store\n",
    "#The search uses Euclidean similarity search, which measures distance between two points in vector space. Measures how similar vectors are\n",
    "query = \"what are the strengths of a vector database\"\n",
    "#query = \"what did president obama say about the nations strengths?\"\n",
    "\n",
    "query_sim_kdbai = vec_db_kdbai.similarity_search(query) #query_sim holds results of the similarity search, the closest related chunks to the query.\n",
    "print(query_sim_kdbai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e414ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response: \n",
      " Vector databases offer significant advantages over traditional scalar-based databases and standalone vector indexes. They are equipped with a set of capabilities that makes them better qualified to be used in high scale production settings, such as scalability and flexibility, API and SDKs, backups and collections, and audit capabilities. They also have the ability to speed up filtering tasks and provide efficient and relevant query results. \n",
      "\n",
      "HuggingFaceHub Response: \n",
      "they offer significant advantages over traditional scalar-based databases and standalone vector indexes\n"
     ]
    }
   ],
   "source": [
    "#run the chain on the query and the related chunks from the documentation\n",
    "print(\"OpenAI Response: \")\n",
    "print(chain_openAI.run(input_documents=query_sim_kdbai, question=query),'\\n')\n",
    "print(\"HuggingFaceHub Response: \")\n",
    "print(chain_HuggingFaceHub.run(input_documents=query_sim_kdbai, question=query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
